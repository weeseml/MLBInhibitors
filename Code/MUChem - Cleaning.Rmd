---
title: "MUChem Documentation"
author: "Mitch Fairweather, Amy Hu"
date: "2/24/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Reading in the Files and packages

```{r packages, warning = T}
pacman::p_load(tidyverse, readxl, magrittr, DataExplorer,reshape2, ggplot2, sqldf, smotefamily,naniar, caret, doParallel, randomForest, VIM, optmatch, reticulate, pROC, vroom, RVerbalExpressions, reticulate)

conda_list()[[1]][1] %>% 
    use_condaenv(required = TRUE)

```

```{r ReadingData}
DataSetFull <- vroom("C:\\MUChemistry\\DataSets\\Original Data\\FullDataSet.csv", col_names = T)

colnames(DataSetFull) <- c("MoleculeName", "Structure", "CDDNumber", "SMILE", "PlateName", "PlateWell", "RFU", "PercentInh")

DataSetFull %<>% select(SMILE, PercentInh) %<>% subset(!is.na(SMILE)) %<>% subset(!is.na(PercentInh))


IC50DataSet <- vroom("C:\\MUChemistry\\DataSets\\Original Data\\3-9-20CrowderInhibitors.csv")[,c(2:3)]

colnames(IC50DataSet) <- c("SMILE", "IC50")

IC50DataSet %<>% subset(!is.na(SMILE)) %<>% subset(!is.na(IC50))

```
Notes: 
- There are two files that are to be read in preliminarily. These include the two data sets given to us from the MU Chemistry department: the ~1000 observations with IC50 values, and ~44000 observations with Percent Inhibition values. 
### Preliminary Data Cleaning

``` {r PrelimDataCleaning}

# remove > and make them 100
IC50DataSet$IC50<-as.numeric(gsub("[\\>]", "100", IC50DataSet$IC50))
# remove observations greater than -50
DataSetFull %<>% subset(PercentInh > -50)

```

Notes: 
- From the data set with IC50 values as the response variable, there were several observations that were listed as having an IC50 value of ">10" or ">100". We removed this character, and replaced it with a numerical value to ensure these observations are later coded to being "bad". 
- Per Toby, in the data set with RFU and percent inhibition, there are many observations that have negative values for percent inhibition. The values that are less than 50% were measurement errors, so we removed these from the data set. The other negative values are valid, and are 100% known to be "bad" inhibitors. 
- There are 631 missing data points for percent inhibition in our DataSetFull. When subsetting it for values only greater than -50, it automatically removes these without having to specifically filter out NA's. 

### Combining the Two Datasets  

``` {r CombiningDataSets, warning = F}

DataSetFull %>% select("SMILE", "PercentInh") %>%
  mutate(IC50 = NA) %>%
  mutate(Response = ifelse(PercentInh >= 50, "Good", "Bad")) -> SubsetDataSetFull

IC50DataSet %>% mutate(PercentInh = NA) %>% 
  mutate(Response = ifelse(IC50 <= 10, "Good", "Bad"))-> SubsetIC50DataSet

CombineDataSet <- rbind(SubsetDataSetFull, SubsetIC50DataSet)
CombineDataSet$Response %<>% as.factor()


#####################

temp1 <- CombineDataSet[which(duplicated(CombineDataSet$SMILE)),]
temp2 <- subset(CombineDataSet, SMILE %in% temp1$SMILE)

yGood <- subset(temp2, Response == "Good")
yBad <- subset(temp2, Response == "Bad")

BadSmiles <- vector()
i <- seq(1:nrow(yGood))

for (val in i) {
  if(is_in(yGood$SMILE[i], yBad$SMILE)){
    BadSmiles[i] = yGood$SMILE[i]
  }
}

BadSmiles %<>% as.data.frame() %>% distinct()
colnames(BadSmiles) <- "SMILE"

CombineDataSet <- subset(CombineDataSet, !(SMILE %in% BadSmiles$SMILE))

```

### Cut-Off Values To Determine "Good" and "Bad" 
The cut-off to determine "good" for SMILES with IC50 is less than or equal to 10. The cut-off to determine "good" for SMILES with  percent inhibition is greater than or equal to 50%. We removed SMILES with RFU and percent inhibition less than or equal to -50%.

### Missing Response Variable or SMILE 
``` {r MissingResponseSMILE}

CombineDataSet %<>% select(SMILE, Response)
CombineDataSet %<>% na.omit()


# This combine data set is what needs to be run through Alvadesc.
# write.table(CombineDataSet[,1], file =
#         "C:\\MUChemistry\\DataSets\\AlvadescImport.txt",
#        sep = ",",
#       col.names = F,
#      row.names = F)

```


Notes: 
- In the full data set with RFU and Percent inhibition, there was a single observation that had a missing SMILE. We removed this. 
- There are also many observations that had missing PercentInhibition or missing IC50 values. We removed these as well. 

### Combining AlvaDesc Descriptors with the Response 
```{r CombiningAlvaDesc}
AlvadescData <- vroom("C:\\MUChemistry\\DataSets\\AlvaDescExport (3-10-20).txt",delim = "\t",na = c("na", "NA", "Na", "", " "))

AlvadescWithResponse <- cbind(CombineDataSet[,c(1:2)], AlvadescData) #Combine the SMILE code and Response


# Have to get rid of the No. and Name columns. These columns are causing the duplicate rows not to be removed, which is problematic. 
AlvadescWithResponse %<>% select(-No., -NAME)

AlvadescWithResponse %<>% distinct()

```

Notes:
- At this point, we need to read in the file outputted from Alvadesc. 
- When Alvadesc creates the output file, it does not print the "SMILE" code along with the 3500 descriptors. Instead, it only prints out an identifier of "Molecule1", or "Molecule2". We handchecked the data, and verified that Alvadesc prints out the data in the same order it was put in. We did this by hand testing random compounds in Mordred, an open source equivalent to Alvadesc, and then compared the calculated Molecular Weights. Checking the first, the last, and random ones in between, the order is the same. 
- We then created a similar unique identifier in our data set with the "good" and "bad" response code, and merged the two. 
- The result is a data set containing all 3500 descriptors and their values from Alvadesc, along with the SMILE code, and the binary response of it being a "good" or "bad" inhibitor.

### Removing Null and Zero Variance Descriptors from AlvaDesc
```{r}

#This below is very necessary to perform PCA. cant scale data with zero variance. 
# returns vectors of column numbers with near zero variance. We will need to remove these to perform a PCA.
removeBadColumns <- function(dataSet, numMissing) {
  
  numericCols = sapply(dataSet, is.numeric)
  zeroVarColumns = nearZeroVar(dataSet[,numericCols], names = T)
  dataSet <- select(dataSet, -zeroVarColumns)

  missingProfile <- profile_missing(dataSet)

  missingProfileMissing <- subset(missingProfile, num_missing > numMissing)

  colNames <- as.vector(missingProfileMissing$feature)

  dataSet %<>% select(-colNames)
  
  return(dataSet)
  
}

AlvadescWithoutZeroVar <- removeBadColumns(AlvadescWithResponse, 506)

#write.csv(AlvadescWithoutZeroVar, "C:\\MUChemistry\\DataSets\\AlvadescWithoutZeroVar.csv")

#save.image("C:\\MUChemistry\\MitchAmyMeeting.rdata")

```

Notes: 
- Alvadesc is only able to pull 2Dimensional descriptors from the SMILE codes we inputted. All of the 3Dimensional descriptors are null, so we removed these from our data set. 
- I believe there is a setting within Alvadesc that creates a text file with only the descriptors which it can calculate. If there are empty columns, the code above will remove those. 
- There are a lot of columns that have zero variance, ie are all one value. These columns wont be useful in prediction, and cannot be included in a PCA analysis as they can't be scaled. 

### Handling Missing Data
```{r MissingData}
missingValues <- function(dataSet) {
  
  colNames <- colnames(dataSet)

  pb1 <- winProgressBar(title = "Loop 1 Progress", min = 0,
                       max = length(colNames), width = 300)
  
  
  for (i in seq(1:length(colNames))){
    setWinProgressBar(pb1, i, title=paste( round(i/length(colNames)*100, 0),
                                          "% done"))
    if(sum(is.na(dataSet[,i])) > 0){

      missingList = rep(0, nrow(dataSet))
      missingObs <- which(is.na(dataSet[,i]))
      meanCol = mean(dataSet[-missingObs,i])

      missingList[missingObs] = 1
      dataSet[missingObs,i] = meanCol
      
      
      
      dataSet <- cbind(dataSet, missingList)
      dataSet$missingList %<>% as.factor()
      colnames(dataSet)[colnames(dataSet) == 'missingList'] <- paste(colNames[[i]], ".NA", sep = "")
    }
    else {
      
      next
      
    }
  }
  return(dataSet)
  
}

AlvadescWithoutZeroVar <- missingValues(AlvadescWithoutZeroVar)


missingProfileAfterImputation <- profile_missing(AlvadescWithoutZeroVar)

## REMOVE DUPLICATE SMILES AND ROWS.
AlvadescWithoutZeroVar %<>% distinct()

#Writing the Alvadescwithoutzerovar to a cSV for RDKIT Scaffolding
write.table(AlvadescWithoutZeroVar[,1], "C:\\MUChemistry\\DataSets\\ScaffoldImport.txt", row.names = F, col.names = F)


```

Notes:

- First, we found there is a systematic grouping of missing data. In total there were 53 unique groups in the full data set. 
- There was a natural cut-off of 506 missing rows in a column. After 506, it jumped to 1000+ missing rows and more. 
- For columns missing more than 506 rows, we removed them from the dataset. 
- For columns with 506 and less missing, we created indicator variables. 1 for missing and 0 for not missing. 
- Next, we imputed the columns with 506 and less missing by taking the mean of the entire dataset. Since 506 and less observations out of 43300+ observations constitutes for less than 1% of the data, we imputed the columns. 
- There are instances of SMILES appearing multiple times in our original data set. For rows that had conflicting responses, such as the same SMILE appearing both good and bad, we removed these earlier. However, there are instances where the same SMILE had the same response code, thus having all of the same predictors. They were identical rows. Running the distinct() function removes the duplicates. 

### Scaffolding (Murcko Scaffolds) HAVE TO RUN IN JUPYTER
```{python Scaffolding}
import rdkit
from rdkit.Chem.Scaffolds import MurckoScaffold
lineList = [line.rstrip('\n') for line in open(r"C:\MUChemistry\DataSets\ScaffoldImport.txt", "r")]

for i in range(0,len(lineList)):
    lineList[i] = lineList[i].strip(' \" ')
    lineList[i] = r'{}'.format(lineList[i])
    
scaffoldList = [""] * len(lineList)
mol = ""

for i in range ( 0 , len(lineList)):
    mol = rdkit.Chem.MolFromSmiles(lineList[i])
    
    if str(type(mol)) == "<class 'NoneType'>":
        scaffoldList[i] = "Unable to Calculate"
        print(i)
        continue
    
    core = MurckoScaffold.GetScaffoldForMol(mol)
    scaffold = rdkit.Chem.MolToSmiles(core)
    
    if scaffold == '':
        scaffoldList[i] = "No Rings"
        continue
        
    scaffoldList[i] = scaffold

results = list(zip(lineList, scaffoldList))

import pandas as pd

results_DF = pd.DataFrame(results, columns = ["SMILE", "ScaffoldSMILE"])

results_DF.to_csv(r"C:\MUChemistry\DataSets\ScaffoldOutput.csv", index = False)

```


``` {r joiningScaffoldsToOriginal}
ScaffoldOutput <- vroom("C:\\MUChemistry\\DataSets\\ScaffoldOutput.csv")
AlvadescScaffold <- cbind(AlvadescWithoutZeroVar,ScaffoldOutput[,2]) %>% distinct()#Joining the scaffold smile
AlvadescScaffold$Response %<>% as.factor()



# Below is used to identify the duplicate smiles. This was done for exploratory purposes to hand check which were duplicate from the original data. This is taken care of by using distinct() from above. 
#which(duplicated(AlvadescWithoutZeroVar$SMILE.x))
#x <- AlvadescWithoutZeroVar[which(duplicated(AlvadescWithoutZeroVar$SMILE.x)),]
#y <- subset(AlvadescWithoutZeroVar, SMILE.x %in% x$SMILE.x)


```

### Training & HOLDOUT WITH SCAFFOLd, what we will use moving forward
``` {r HoldoutDataScaffolding}
trainTestScaffold <- function(dataSet, groupSize){
  set.seed(2020)
  
  GroupNames <- as.data.frame(unique(dataSet$ScaffoldSMILE)) %>% mutate(GroupNum = 1:n())
  colnames(GroupNames) <- c("ScaffoldSMILE","GroupNum")
  GroupNames$ScaffoldSMILE %<>% as.factor()
  GroupNames$GroupNum %<>% as.factor()
  
  dataSet <- merge(x = dataSet, y = GroupNames, by = "ScaffoldSMILE")
  
  index <- seq(1:nrow(GroupNames))
  groupsTrain <- sample(index, groupSize*length(index), replace = F)
  groupsTest <- index[-groupsTrain]
  
  x <- subset(dataSet, GroupNum %in% groupsTrain) %>% select(-GroupNum)
  y <- subset(dataSet, GroupNum %in% groupsTest) %>% select(-GroupNum)
  
  returnList <- list(trainingSet = x, holdoutSet = y)
  return(returnList)
  
}

scaffoldSets <- trainTestScaffold(AlvadescScaffold, groupSize = .8)

TrainingSetScaffold <- scaffoldSets$trainingSet
HoldoutSetScaffold <- scaffoldSets$holdoutSet

summary(as.factor(TrainingSetScaffold$Response))
summary(as.factor(HoldoutSetScaffold$Response))

```

Using the scaffold smiles we acquired from RDKit in python, we used the number of unique smiles as "groups" of similar compounds. In total, there was 25077(?) in our original data. Using this assumption that these were "groups", we created an index or unique identifier of each group. 

From these group numbers, we split our training and holdout sets using these. We included 80% of the scaffold groups in our training set, and the remaining 20% of the groups in our holdout or out of sample daa set. 


### Holdout WITHOUT SCAFFOLD
``` {r HoldoutDataRandom}

trainTestSets <- function(dataSet, testSize){
  set.seed(2020)
  dataSet$Response = as.factor(dataSet$Response)
  trainRowNums = createDataPartition(dataSet$Response, p=testSize, list = F) %>% as.vector()
  
  x = dataSet[trainRowNums,]
  y = dataSet[-trainRowNums,]
  
  returnList <- list(trainingSet = x, holdoutSet = y)
  return(returnList)
  
}

randSampleSets <- trainTestSets(AlvadescWithoutZeroVar, .8)

TrainingSet <- randSampleSets$trainingSet
HoldoutSet <- randSampleSets$holdoutSet

summary(as.factor(TrainingSet$Response))
summary(as.factor(HoldoutSet$Response))


#save.image("C:\\MUChemistry\\temp.rdata")

```
Notes:

- The holdout is 5000 observations. The approximate proportion of "good" to "bad" is 0.019. We will have 4905 "bad" and 95 "good". - First, we split the full dataset into two datasets. One "good" and the second "bad". Within the "good", we randomly sampled 95 records. Within the "bad", we randomly sampled 4905 records. We did this to ensure the proportion would be representative of the full dataset and randomly sampled for the holdout. 

### Random Forest with Scaffold MITCH

```{r readRForestScaffold1}

RForestScaffold1 <- readRDS("C:\\MUChemistry\\Models\\RForestScaffold1.rds")

```

``` {r RForestScaffold1, eval = FALSE}

# AUC Holdout = 0.8097

#Parallel Processing
cl <- makePSOCKcluster(20)
registerDoParallel(cl)

#setting up training/validation
index<-seq(1:nrow(TrainingSetScaffold))

kvalues<-c(40, 45, 50, 55, 60)
tunegrid <- data.frame(.mtry=kvalues)
set.seed(2020)
cvindx<-createFolds(index, k=5, returnTrain = TRUE)

ctrl <- trainControl(method="cv", index=cvindx, 
                     summaryFunction = twoClassSummary, 
                     classProbs = TRUE, allowParallel = T)

levels(TrainingSetScaffold$Response)

TrainingSetScaffold$Response <- relevel(TrainingSetScaffold$Response, ref = "Good") # Caret is backwards for reference value. 

RForestScaffold1 <-train(x=select(TrainingSetScaffold, -Response, -SMILE,
                                  -ScaffoldSMILE), 
               y=TrainingSetScaffold$Response, 
               data=TrainingSetScaffold, 
               method="rf", 
               tuneGrid=tunegrid, 
               ntree=100, 
               trControl=ctrl)
stopCluster(cl)

RForestScaffold1
# plot(RForestScaffold1)


saveRDS(RForestScaffold1, "C:\\MUChemistry\\Models\\RForestScaffold1.rds")


```

``` {r PredictionsFromRforest}
#AUC on Holdout
p.RForestScaffold1Hold<-predict(RForestScaffold1, newdata = HoldoutSetScaffold, type="prob")
r.RForestScaffold1Hold<-roc(HoldoutSetScaffold$Response,  p.RForestScaffold1Hold[,1])
r.RForestScaffold1Hold.auc<-r.RForestScaffold1Hold$auc
r.RForestScaffold1Hold.auc


### Random Forest without Scaffold
varImpRForestScaffold1 <- varImp(RForestScaffold1)
varImpRForestScaffold1$Importance
# plot(varimptRForest)

```


### Random Forest without Scaffold AMY

```{r}
RForest1 <- readRDS("C:\\MUChemistry\\Models\\RForest1.rds")

```

``` {r RForestNoScaffold1, eval = FALSE}

# AUC Holdout = 0.79

#Parallel Processing
cl <- makePSOCKcluster(20)
registerDoParallel(cl)

#setting up training/validation
index<-seq(1:nrow(TrainingSet))

#Random Forest Model - can't have a variable with more than 53 categories 
kvalues<-c(40, 45, 50, 55, 60)
tunegrid <- data.frame(.mtry=kvalues)
set.seed(2020)
cvindx<-createFolds(index, k=5, returnTrain = TRUE)

ctrl <- trainControl(method="cv", index=cvindx, 
                     summaryFunction = twoClassSummary, 
                     classProbs = TRUE)

# additions made to data for random forest 
levels(TrainingSet$Response)
TrainingSet$Response <- relevel(TrainingSet$Response, ref = "Good")

RForest1 <-train(x=select(TrainingSet, -Response, -SMILE), 
               y=TrainingSet$Response, 
               data=TrainingSet, 
               method="rf", 
               tuneGrid=tunegrid, 
               ntree=100, 
               trControl=ctrl)
RForest1
#plot(RForest1)


varImp(RForest1)
saveRDS(RForest1, "C:\\MUChemistry\\Models\\RForest1.rds")

stopCluster(cl)

```

```{r PredictionsFromRForestReg}

#AUC on Holdout
p.RForest1Hold<-predict(RForest1, newdata = HoldoutSet, type="prob")
r.RForest1Hold<-roc(HoldoutSet$Response,  p.RForest1Hold[,1])
r.RForest1Hold.auc<-r.RForest1Hold$auc
r.RForest1Hold.auc


### Random Forest without Scaffold
```

The above two models were created as our "baseline" for comparing whether scaffolding our data was worth using to create training and holdout sets. Judging by the metric AUC, the scaffolded model performs marginally worst. This was good enough for us to move forward with Scaffolding as it is supported by previous literature. 

### PCA
```{r PCAonScaffoldData}
set.seed(2020)

preProcessTrainScaffold <- preProcess(TrainingSetScaffold, method = c("center","scale"))

ind <- sapply(TrainingSetScaffold, is.numeric)

trainingScaffoldPCA <-prcomp(predict(preProcessTrainScaffold, TrainingSetScaffold)[ind], scale = F, center = F)

#Getting the weights 
#trainingScaffoldPCA$rotation

#summary(trainingScaffoldPCA)
#View(summary(trainingScaffoldPCA)[["importance"]])

```

PCA @ 400 components: 99.125% of variation

Notes: 
- First step we want to do before running our PCA analysis is mean centering the data and to normalize it as well. We will mean center it, and standardize it using the z-score method. 
- Correction to above: we have decided not to scale the entire data set before running the PCA. Instead, we will let PCA scale it for us. Our rational: doing it this way, we can preserve the original data, and avoid mistakenly double or triple scaling the data if a certain model/technique scale it automatically without our knowledge. 

Notes: 
- The first step in running the PCA on our training set was to first create a preprocessing object that would mean center and scale the data. We could then use the values from this to project on to any new data, ensuring we are applying the exact same weights and scaling numbers to any new data set. 

- PCA can only be done on numerical columns. We created an index of the numerical columns in our data set, called "ind". This index will be used for our validation set, our holdout set, and any future data. We are operating under the assumption that any new compounds can be run through alvadesc and will have the exact same columns/predictors. 

- Within the prcomp, we used the predict function to project the center and scaling data we calculated from the TrainingSetScaffold data set on to itself. In effect, this simply just mean centers and then scales it based on each columns standard deviation. From this data set, I indexed for just numeric columns using the "ind" index created above. To prevent our data from being scaled or centered twice, I set these functions within PRComp to false. 

``` {r ProjectingPCATrainingWeights}

projectPCA <- function(dataSet, pcaObject, numComponents, preProcessItem){
  ind <- sapply(dataSet, is.numeric)
  
  pcaWeights <- pcaObject$rotation
  
  dataSetTransformed <- as.matrix(predict(preProcessItem,dataSet)[ind]) %*%
    (pcaWeights[,seq(1:numComponents)])
  dataSetTransformed %<>% as.data.frame()
  
  dataSet <- cbind(dataSet[!ind],dataSetTransformed)
  
  return(dataSet)
  
}

TrainingSetScaffoldPCA <- projectPCA(dataSet = TrainingSetScaffold, 
                                     pcaObject = trainingScaffoldPCA,
                                     numComponents = 400, 
                                     preProcessItem = preProcessTrainScaffold)

```



``` {r ProjectingPCAHoldout}

HoldoutScaffoldPCA <- projectPCA(dataSet = HoldoutSetScaffold, pcaObject = trainingScaffoldPCA,
                                 numComponents = 400,preProcessItem = preProcessTrainScaffold)

write.csv(HoldoutScaffoldPCA, "C:\\MUChemistry\\DataSets\\HoldoutSets\\HoldoutScaffoldPCA.csv")

```
Notes: 
- Within the matrix multiplication, use the preProcessTrain function to center and scale the holdout set based on the values from our training set, and then only pulling the numerical columns from it. 

Now that we have our PCA weights above, we created new Training and holdout sets transformed by the first 400 PCA components. We decided to start with 400 components because this was the lowest number of components we could go and still keep 99% of the total variation. We had hoped we could reduce the dimension further, but we will move forward with the 400 components. 


### Random Forest on PCA
```{r}
RForestScaffoldPCA1 <- readRDS("C:\\MUChemistry\\Models\\RForestScaffoldPCA1.rds")
```

``` {r RForestPCA, eval = FALSE}

# AUC HoldoutScaffoldPCA = 0.723

#Parallel Processing
set.seed(2020)
cl <- makePSOCKcluster(20)
registerDoParallel(cl)

#setting up training/validation
index<-seq(1:nrow(TrainingSetScaffoldPCA))

kvalues<-c(40, 45, 50, 55, 60)
tunegrid <- data.frame(.mtry=kvalues)
cvindx<-createFolds(index, k=10, returnTrain = TRUE)

ctrl <- trainControl(method="cv", index=cvindx, 
                     summaryFunction = twoClassSummary, 
                     classProbs = TRUE)

levels(TrainingSetScaffoldPCA$Response)
TrainingSetScaffoldPCA$Response <- relevel(TrainingSetScaffoldPCA$Response, ref = "Good")

RForestScaffoldPCA1 <-train(x=select(TrainingSetScaffoldPCA, -Response, -SMILE, -ScaffoldSMILE),
               y=TrainingSetScaffoldPCA$Response, 
               data=TrainingSetScaffoldPCA, 
               method="rf", 
               metric = "ROC",
               tuneGrid=tunegrid, 
               ntree=100, 
               trControl=ctrl)
RForestScaffoldPCA1

varImp(RForestScaffoldPCA1)
saveRDS(RForestScaffoldPCA1, "C:\\MUChemistry\\Models\\RForestScaffoldPCA1.rds")

stopCluster(cl)

```

``` {r PredFromPCARForest}
#AUC on Holdout
p.RForestScaffoldPCA1Hold<-predict(RForestScaffoldPCA1, newdata = HoldoutScaffoldPCA , type="prob")
r.RForestScaffoldPCA1Hold<-roc(HoldoutScaffoldPCA$Response,  p.RForestScaffoldPCA1Hold[,1])
r.RForestScaffoldPCA1Hold.auc<-r.RForestScaffoldPCA1Hold$auc

r.RForestScaffoldPCA1Hold.auc

varImp(RForestScaffoldPCA1)

```

```{r filesForSmote}
write.csv(TrainingSetScaffoldPCA, file = "C:\\MUChemistry\\DataSets\\TrainingSetScaffoldPCA.csv",
          row.names = F)

write.csv(TrainingSetScaffold, file = "C:\\MUChemistry\\DataSets\\TrainingSetScaffold.csv",
          row.names = F)


```

### SMOTE NC in Python
``` {python}
import pandas as pd
import numpy as np
from imblearn.over_sampling import SMOTENC, ADASYN

#Load the file into a pandas data frame.

def SMOTE(filePath, trainStart, trainEnd, firstCatFeature, lastCatFeature, response, dupSize):
    data = pd.read_csv(filePath)
    
    X_train = data.iloc[:,trainStart:trainEnd + 1]
    y_train = data.iloc[:,response]
    
    smote_nc = SMOTENC(categorical_features=list(np.arange(firstCatFeature - trainStart, 1 + lastCatFeature -trainStart)), sampling_strategy = ((np.unique(y_train, return_counts=True)[1][1]) * dupSize /  (np.unique(y_train, return_counts=True)[1][0])),random_state=2020)
    
    X_smoted, y_smoted = smote_nc.fit_resample(X_train, y_train)
    
    SmoteDataFrame = X_smoted.copy(deep = True)
    SmoteDataFrame['Response'] = y_smoted
    
    return SmoteDataFrame

def saveSmoteCSV(exportPath, dataFrame):
    dataFrame.to_csv(exportPath, index = False)
    
    return "Finished"


TrainingSetScaffoldPCASMOTE = SMOTE(filePath = "C:\MUChemistry\DataSets\TrainingSetScaffoldPCA.csv", trainStart = 3, trainEnd = 776, firstCatFeature = 3, lastCatFeature = 376, response = 2,dupSize = 3)
                                    
saveSmoteCSV(exportPath = r"C:\MUChemistry\DataSets\TrainingSetScaffoldPCASMOTE.csv", dataFrame = TrainingSetScaffoldPCASMOTE) 

                                    
TrainingSetScaffoldSMOTE = SMOTE(filePath= "C:\MUChemistry\DataSets\TrainingSetScaffold.csv", trainStart = 3, trainEnd = 2242, firstCatFeature = 1869, lastCatFeature= 2242, response = 2, dupSize= 3)
  
saveSmoteCSV(exportPath= r"C:\MUChemistry\DataSets\TrainingSetScaffoldSMOTE.csv", dataFrame = TrainingSetScaffoldSMOTE)

```

Due to severe class imbalance, we used SMOTE NC in python to synthetically over sample the data. We attempted SMOTE in r, but the algorithm is unable to handle categorical predictors. 

``` {r readingSmoteData}
TrainingSetScaffoldPCASMOTE <- vroom("C:\\MUChemistry\\DataSets\\TrainingSetScaffoldPCASMOTE.csv")

ind <- sapply(colnames(TrainingSetScaffoldPCASMOTE),
              function(x){ifelse(substr(x,(nchar(x)+1)-3,nchar(x)) == ".NA", T, F)})

TrainingSetScaffoldPCASMOTE[ind] <- lapply(TrainingSetScaffoldPCASMOTE[ind], function(x){as.factor(x)})

TrainingSetScaffoldSMOTE <- vroom("C:\\MUChemistry\\DataSets\\TrainingSetScaffoldSMOTE.csv")

ind <- sapply(colnames(TrainingSetScaffoldSMOTE),
              function(x){ifelse(substr(x,(nchar(x)+1)-3,nchar(x)) == ".NA", T, F)})

TrainingSetScaffoldSMOTE[ind] <- lapply(TrainingSetScaffoldSMOTE[ind], function(x){as.factor(x)})

```

``` {r create50/50ResponseSamples}

sampleCreator <- function(dataSet){
  
  goodData <- subset(dataSet, Response == "Good")
  badData <- subset(dataSet, Response == "Bad")
  index <- seq(1:nrow(badData))
  
  vec <- list()
  dataFrameList <- list()
  
  numSamples <- floor(nrow(badData) / nrow(goodData))
  
  pb1 <- winProgressBar(title = "Progress", min = 0,
                       max = numSamples, width = 300)
    
  for (val in seq(1:numSamples)) {
    
    tempSample <- sample(index, size = nrow(goodData), replace = FALSE)
    index <- index[-tempSample]
    tempDataFrame <- badData[tempSample,]
    goodDataFrame <- goodData
    
    
    tempDataFrame2 <- rbind(tempDataFrame,goodDataFrame)
    dataFrameList[[val]] <- tempDataFrame2
    vec[[val]] <- tempSample
    setWinProgressBar(pb1, val, title=paste( round(val/numSamples*100, 0),
                                          "% done"))
  } 
  return(dataFrameList)
  
}

TrainingSetScaffoldPCASmoteList = sampleCreator(TrainingSetScaffoldPCASMOTE)

TrainingSetScaffoldSmoteList = sampleCreator(TrainingSetScaffoldSMOTE)

```

For model building, we wanted to have a 50/50 split of good and bad data. However, even with SMOTE'd data, we only had 1642 total good observations, and about 36000 bad data. Our solution to this is to split our 36000 bad data in to 20 different groups of 1642 observations, and join the 1642 good observations to them. Next, we will build a model on each, and ensemble all of them together.

At this point, we have all of the data sets we will need moving forward for any model (we think). We have: 
1. A standard training and holdout set created directly from Alvadesc Output created using random sampling with an 80/20 split
2. A training and holdout set created from the Alvadesc Output created using an 80/20 split of the Murcko Scaffold groups. There were about 25,000 groups in total. 80% of those groups would be training, and any compound that belonged to one of those groups would be a training observation. 
3. A training and holdout set with PCA projected on to the Murcko Scaffolded training set. We elected to keep 400 components, as that allowed us to keep 99% of the original variation. 
4. A SMOTED training set, done on the PCA training set. The out of sample holdout set we will use is the original PCA Holdout set created in the step above. 
5. A set of 20 different samples from the SMOTED training set. We took all of the good observations, about 1600, and combined that with a random sample of 1600 bad sampled without replacement. 

Below, we are simply writing all of these to 

``` {r, eval = FALSE}
#Step 1 
write.csv(TrainingSet, "C:\\MUChemistry\\DataSets\\TrainingSets\\TrainingSet.csv")
write.csv(HoldoutSet, "C:\\MUChemistry\\DataSets\\HoldoutSets\\HoldoutSet.csv")

#Step 2
write.csv(TrainingSetScaffold, "C:\\MUChemistry\\DataSets\\TrainingSets\\TrainingSetScaffold.csv")
write.csv(HoldoutSetScaffold, "C:\\MUChemistry\\DataSets\\HoldoutSets\\HoldoutSetScaffold.csv")

#Step 3
write.csv(TrainingSetScaffoldPCA,
          "C:\\MUChemistry\\DataSets\\TrainingSets\\TrainingSetScaffoldPCA.csv")
write.csv(HoldoutScaffoldPCA,
          "C:\\MUChemistry\\DataSets\\HoldoutSets\\HoldoutScaffoldPCA.csv")

#Step 4
write.csv(TrainingSetScaffoldPCASMOTE,
          "C:\\MUChemistry\\DataSets\\TrainingSets\\TrainingSetScaffoldPCASMOTE.csv")
write.csv(TrainingSetScaffoldSMOTE,
          "C:\\MUChemistry\\DataSets\\TrainingSets\\TrainingSetScaffoldSMOTE.csv")



#Step 5
for(i in seq(1:length(TrainingSetScaffoldSmoteList))){
  write.csv(TrainingSetScaffoldSmoteList[[i]],
            paste("C:\\MUChemistry\\DataSets\\TrainingSets\\TrainingSetScaffoldSmoteList\\", "Sample", i, ".csv",sep = ""))
}
  
for(i in seq(1:length(TrainingSetScaffoldPCASmoteList))){
  write.csv(TrainingSetScaffoldPCASmoteList[[i]],
            paste("C:\\MUChemistry\\DataSets\\TrainingSets\\TrainingSetScaffoldPCASmoteList\\", "Sample", i, ".csv",sep = ""))
  
}

```

``` {r findingXGBTuningParams}

grid <- read.csv("C:\\MUChemistry\\Models\\xgboostGrid.csv") %>% 
  mutate(.min_child_weight = 1)

paramList = list()
j = 1
k = 1

maxModels <- nrow(grid) * length(TrainingSetScaffoldPCASmoteList)

for (i in seq(1:maxModels)){
    paramList[[i]] = list()
    paramList[[i]][[1]] = paste("Sample", k, sep = "")
    paramList[[i]][[2]] = paste("Model", j, sep = "")
    paramList[[i]][[3]] = grid[j,1]
    paramList[[i]][[4]] = grid[j,2]
    paramList[[i]][[5]] = grid[j,3]
    paramList[[i]][[6]] = grid[j,4]
    paramList[[i]][[7]] = grid[j,5]
    paramList[[i]][[8]] = grid[j,6]
    paramList[[i]][[9]] = grid[j,7]
    j = j + 1

    if (j == nrow(grid) +1){
      j = 1
      k = k + 1
    }
    if (k == length(TrainingSetScaffoldPCASmoteList) + 1){
      k = 1
    }
      
}

xgbGrid <- data.frame(do.call(rbind, paramList))
colnames(xgbGrid) <- c("SampleNum", "ModelNum", ".gamma", ".nrounds", ".max_depth", ".eta", ".colsample_bytree", ".subsample",".min_child_weight")

#xgbGrid %<>% select(SampleNum, ModelNum, .nrounds, .max_depth, .eta, .gamma, .colsample_bytree, .min_child_weight, .subsample)

xgbGrid[c(1:2)] <- sapply(xgbGrid[c(1:2)],as.character)
xgbGrid[c(3:9)] <- sapply(xgbGrid[c(3:9)],as.numeric)

#Have to round nrounds and max depth to a whole number for caret

xgbGrid %<>% mutate_at(vars(.nrounds, .max_depth), round)


cl <- makePSOCKcluster(35)
registerDoParallel(cl)

fitControl = trainControl(
  method = "cv", # k-fold cross validation
  number = 10, # Number of Folds
  search = "grid", # grid search for parameter tuning when applicable
  summaryFunction = twoClassSummary, # see custom_functions
  classProbs = T, # should class probabilities be returned
  selectionFunction = "best", # best fold
  savePredictions = 'all', 
  allowParallel = T)

trainSampleTracker = 1
giantModelList <- list()


for (i in seq(1:nrow(xgbGrid))){
  
  trainSample <- TrainingSetScaffoldPCASmoteList[[trainSampleTracker]]
  trainSample$Response %<>% as.factor()
  trainSample$Response <- relevel(trainSample$Response, ref = "Good")

  
  tempModel <- train(Response ~ ., data = trainSample, 
                     family = "binomial",
                     metric = "ROC", 
                     method = "xgbTree", 
                     tuneGrid = xgbGrid[i,c(3:9)], 
                     trControl = fitControl)
  
  giantModelList[[i]] <- tempModel
  saveRDS(tempModel, paste("C:\\MUChemistry\\Models\\540Models\\", 
                           "ModelNum", i, ".rds", sep = ""))
  if(i %% nrow(grid) == 0){
    trainSampleTracker = trainSampleTracker + 1
  }
  
}

stopCluster(cl)


```

``` {r PredAndAUCFrom612Models}
xgbTreeDataPrep <- function(dataSet, modelObject){
  
  dataSet$Response %<>% as.factor()
  
  modelObjectColNames <- colnames(modelObject$trainingData)
  dataSetColNames <- colnames(dataSet)
  
  `%notin%` <- Negate(`%in%`)
  
  for (i in dataSetColNames) {
    if (i == "Response"){
      next
    }
    else if (i %in% modelObjectColNames){
      next
    }
    else if (i %notin% modelObjectColNames){
      dataSet = select(dataSet, -i)
    }
    
  }
  
  dataSet <-model.matrix(object = Response~., data = dataSet)
  dataSet <- dataSet[,-1]
  
  dataSetMatrixColNames <- colnames(dataSet)
  modelCoefNames <- modelObject$coefnames
  
  tracker = 1
  
  for (i in modelCoefNames) {
    if (i %in% dataSetMatrixColNames){
      tracker = tracker + 1
      next
    }
    else if (i %notin% dataSetMatrixColNames){
      dataSet = dataSet[,-tracker]
    }
    
  }

  dataMatrix <- xgboost::xgb.DMatrix(dataSet, missing = NA)
  return(dataMatrix)
  
}

## Get predictions from each model for each training set

trainSampleTracker = 1
predListTrain <- list()


for (i in seq(1:length(giantModelList))){
  predDataTrain <- TrainingSetScaffoldPCASmoteList[[trainSampleTracker]]
  
  predDataTrainDMatrix <- xgbTreeDataPrep(dataSet = predDataTrain, 
                                          modelObject = giantModelList[[1]])
  
  tempPred<-predict(giantModelList[[i]]$finalModel, newdata = predDataTrainDMatrix)
  predListTrain[[i]] <- tempPred
  
  
  if(i %% nrow(grid) == 0){
    trainSampleTracker = trainSampleTracker + 1
  }

}

## USe those predictions to calculate an AUC value 

aucPredListTrain <- list()

trainSampleTracker = 1

for(i in seq(1:length(predListTrain))){
  tempROC = roc(TrainingSetScaffoldPCASmoteList[[trainSampleTracker]]$Response,
                predListTrain[[i]])
  tempAUC = auc(tempROC)
  aucPredListTrain[[i]] = tempAUC
  
  if(i %% nrow(grid) == 0){
    trainSampleTracker = trainSampleTracker + 1
  }
}

aucDFTrain <- data.frame(do.call(rbind, aucPredListTrain))

### getting predictions on the holdout data for each model
predListHoldout <- list()

predDataHoldoutDMatrix <- xgbTreeDataPrep(dataSet = HoldoutScaffoldPCA, 
                                          modelObject = giantModelList[[1]])

for (i in seq(1:length(giantModelList))){
  tempPred<-predict(giantModelList[[i]]$finalModel, newdata = predDataHoldoutDMatrix)
  predListHoldout[[i]] <- tempPred
}


### Getting auc on holdout data for each model

aucPredListHoldout <- list()

for(i in seq(1:length(predListHoldout))){
  tempROC = roc(HoldoutScaffoldPCA$Response, predListHoldout[[i]])
  tempAUC = auc(tempROC)
  aucPredListHoldout[[i]] = tempAUC
}

aucDFHold <- data.frame(do.call(rbind, aucPredListHoldout))

### Combining the training, holdout auc with the original tuning parameters

xgbGridAUC <- cbind(xgbGrid, aucDFTrain, aucDFHold)

colnames(xgbGridAUC) <- c("SampleNum", "ModelNum", ".gamma", ".nrounds",  ".max_depth",
                          ".eta", ".colsample_bytree", ".subsample", 
                          ".min_child_weight", "AucTrain", "AucHoldout")

#write.csv(xgbGridAUC, "C:\\MUChemistry\\Models\\TuningParamAucGrid.csv", row.names = F)


```

### Optimal parameters: 

Gamma: 47
Nrounds: 400
maxDepth: 20
Eta: .01
Colsample by tree: .2
Subsample: 0.8


``` {r ReRunningXgbTreeAfterTuning}

######################## creating all 20 models #################
fitControl = trainControl(
  method = "cv", # k-fold cross validation
  number = 10, # Number of Folds
  search = "grid", # grid search for parameter tuning when applicable
  summaryFunction = twoClassSummary, # see custom_functions
  classProbs = T, # should class probabilities be returned
  selectionFunction = "best", # best fold
  savePredictions = 'all', 
  allowParallel = T)



tunedXgbGrid <- expand.grid(.nrounds = 400, 
                    .max_depth = 20, 
                    .eta = .01,
                    .gamma = 47, 
                    .colsample_bytree = .2,
                    .min_child_weight = 1,
                    .subsample = 0.8
)


tunedModelList <- list()

cl <- makePSOCKcluster(30)
registerDoParallel(cl)

pb1 <- winProgressBar(title = "Progress", min = 0,
                       max = length(TrainingSetScaffoldPCASmoteList), width = 300)

for(i in seq(1:length(TrainingSetScaffoldPCASmoteList))){
  trainSample <- TrainingSetScaffoldPCASmoteList[[i]]
  trainSample$Response %<>% as.factor()
  trainSample$Response <- relevel(trainSample$Response, ref = "Good")

  
  
  tempModel <- train(Response ~ ., data = trainSample, 
                     family = "binomial",
                     metric = "ROC", 
                     method = "xgbTree", 
                     tuneGrid = tunedXgbGrid, 
                     trControl = fitControl)
  
  tunedModelList[[i]] <- tempModel
  saveRDS(tempModel, paste("C:\\MUChemistry\\Models\\TunedModelsOn18Samples\\", 
                           "ModelSample", i, ".rds", sep = ""))
  setWinProgressBar(pb1, i, title=paste(round(i/length(TrainingSetScaffoldPCASmoteList)*100, 0),
                                          "% done"))
  
}

stopCluster(cl)
saveRDS(tunedModelList, "C:\\MUChemistry\\Models\\tunedModelList.rds")

```

``` {r}

#Prediction on Holdout data
predListHoldoutTuned <- list()

predDataHoldoutDMatrix <- xgbTreeDataPrep(dataSet = HoldoutScaffoldPCA, modelObject = tunedModelList[[2]])

for (i in seq(1:length(tunedModelList))){
  tempPred<-predict(tunedModelList[[i]]$finalModel, newdata = predDataHoldoutDMatrix)
  predListHoldoutTuned[[i]] <- tempPred
}

aucPredListHoldoutTuned <- list()

for(i in seq(1:length(predListHoldoutTuned))){
  tempROC = roc(HoldoutScaffoldPCA$Response, predListHoldoutTuned[[i]])
  tempAUC = auc(tempROC)
  aucPredListHoldoutTuned[[i]] = tempAUC
}

aucDFHoldTuned <- data.frame(do.call(rbind, aucPredListHoldoutTuned))

## Overall average predictions 

averagePredListTuned <- list()

for(i in seq(1:nrow(HoldoutScaffoldPCA))){
  runningTotal <- 0
  
  for(k in seq(1:length(predListHoldoutTuned))){
    runningTotal <- runningTotal + predListHoldoutTuned[[k]][i]
  }
  averagePred <- runningTotal / length(predListHoldoutTuned)
  averagePredListTuned[[i]] <- averagePred
  
}

r.BoostedPred<-roc(HoldoutScaffoldPCA$Response,  as.numeric(averagePredListTuned))
r.BoostedPred.AUC<-r.BoostedPred$auc
r.BoostedPred.AUC


```

``` {r}
#Predictions on Training Data
predListTrainingTuned <- list()


for (i in seq(1:length(tunedModelList))){
  predDataTrainDMatrix <- xgbTreeDataPrep(dataSet = TrainingSetScaffoldPCASmoteList[[i]],
                                          modelObject = tunedModelList[[1]])
  tempPred <- predict(tunedModelList[[i]]$finalModel, newdata = predDataTrainDMatrix)
  predListTrainingTuned[[i]] <- tempPred
}

aucPredListTrainingTuned <- list()

for(i in seq(1:length(predListTrainingTuned))){
  tempROC = roc(TrainingSetScaffoldPCASmoteList[[i]]$Response, predListTrainingTuned[[i]])
  tempAUC = auc(tempROC)
  aucPredListTrainingTuned[[i]] = tempAUC
}

aucDFTrainTuned <- data.frame(do.call(rbind, aucPredListTrainingTuned))

```

``` {r Lift&Gain}
liftData <- HoldoutScaffoldPCA
predDataHoldoutDMatrix <- xgbTreeDataPrep(dataSet = liftData, modelObject = tunedModelList[[1]])

liftPreds <- predict(tunedModelList[[6]]$finalModel, newdata = predDataHoldoutDMatrix, type = "prob")
liftData$prob1 <- liftPreds

results <- select(liftData, c(Response, prob1))

predObject<-prediction(results$prob1, results$Response)

lift<-performance(predObject, "lift", "rpp")
plot(lift,main="Lift chart", col="brown1")

gain<-performance(predObject, "tpr", "rpp")
plot(gain,main="Gain chart", col="brown1")

```

``` {r LiftTable}
results$Response %<>% as.factor()
results$Response %<>% relevel(ref = "Good")
liftTable <-lift(Response~prob1 , data=results)



```

It looks like the cutoff value I will recommend is about .5388 and above to be classified as a 1, and thus be tested by the chemists. At this point, we will find approximately 50% of the overall good compounds in the data, and only have to test about 12% of the total data set. 

Model 1 cutoff: 0.538
Model 2 cutoff: 0.519
Model 3 cutoff: 0.525
Model 4 Cutoff: 0.521
Model 5 cutoff: 0.521 
Model 6 cutoff: 0.528


#Predictions on NIH New data

``` {r NIH}
NIH70kData <- vroom("C:\\MUChemistry\\DataSets\\SecondNIHLibrary.txt", delim = "\t", na = c("NA","Na", "na", ""))
NIH70kData.smiles <- vroom("C:\\MUChemistry\\DataSets\\SecondNIHLibrarySMILES.txt", col_names = F, delim = "\t", na = c("NA","Na", "na", ""))
colnames(NIH70kData.smiles) <- c("SMILE")
NIH70kData <- cbind(data.smiles, data)

NIH70kData %<>% 
  select_if(names(.) %in% c(colnames(AlvadescWithoutZeroVar))) # Removing any columns that were removed due to them being zero variance in the original data set

NIH70kData <- missingValues(dataSet = NIH70kData) # Imputing values for missing observations and creating missing value indicator columns

#Using the TrainingSet, which is the original raw data that was partitioned by random sample, I need to check to see if there were any missing value indicator columns in the original data that was not just created by the missingValues function. Below is accomplishing that. 

TrainSetCols <- as.vector(colnames(TrainingSet))
NewDataCols <- as.vector(colnames(NIH70kData))
`%notin%` <- Negate(`%in%`)


for (i in TrainSetCols) {
  if (i %in% NewDataCols){
    next
  }
  else if (i == "Response"){
    next
  }
  else if (i %notin% NewDataCols){
    tempList <- rep(0, nrow(NIH70kData)) #Calling everything in our new data as False or not missing
    NIH70kData <- cbind(NIH70kData, tempList)
    NIH70kData$tempList %<>% as.factor()
    levels(NIH70kData$tempList) <- c(0,1)
    colnames(NIH70kData)[colnames(NIH70kData) == 'tempList'] <- i
  }
  
}

NIH70kData %<>% distinct() #Just to ensure there are no duplicates. There were 76369 observations before, and 76168 after. Indicates there were some duplicate SMILES in the original data. This isn't concerning, but just needs to be removed. 

#Projecting PCA Training weights on to it

NIH70kDataPCA <- projectPCA(dataSet = NIH70kData, pcaObject = trainingScaffoldPCA, numComponents = 400, preProcessItem = preProcessTrainScaffold)

View(lapply(NIH70kDataPCA, class))


NIHDataMatrix <- model.matrix(object = ~., data = NIH70kDataPCA[,-1]) #Building the model matrix on the data less the SMILE column
NIHDataMatrix <- NIHDataMatrix[,-1] #Removing the intercept column

#Predictions wont work if the columns are in a different order than the coefficients in the model itself. This reorders the columns in the data matrix to match that of the coefficients in the model
NIHDataMatrix <- NIHDataMatrix[, match(tunedModelList[[1]]$coefnames, colnames(NIHDataMatrix))]

### Ready for Predictions 

predListNIH70K <- list()

for (i in seq(1:length(tunedModelList))){
  tempPred<-predict(tunedModelList[[i]]$finalModel, newdata = NIHDataMatrix)
  predListNIH70K[[i]] <- tempPred
}

averagePredListNIH70k <- list()

for(i in seq(1:nrow(NIH70kDataPCA))){
  runningTotal <- 0
  
  for(k in seq(1:length(predListNIH70K))){
    runningTotal <- runningTotal + predListNIH70K[[k]][i]
  }
  averagePred <- runningTotal / length(predListNIH70K)
  averagePredListNIH70k[[i]] <- averagePred
  
}

finalPredictions <- data.frame(do.call(rbind, averagePredListNIH70k))

NIH70KPredictions <- NIH70kData
NIH70KPredictions$prob1 <- finalPredictions

## Subsetting the data set for compounds above our threshold from the first model 
NIH70kDataForTesting <- subset(NIH70KPredictions, prob1 > .525)

```

### Clustering the NIH Data


``` {r Clustering on NIHData}
RForest1 <- readRDS("C:\\MUChemistry\\Models\\RForest1.rds")

#Variables to use are the top varims from the RForest built on all variables
varImpScaffold <- varImp(RForest1)$importance
varImpScaffold <- cbind(rownames(varImpScaffold), varImpScaffold)

colnames(varImpScaffold) <- c("Feature", "Importance")
varImpScaffold <- varImpScaffold[order(-varImpScaffold$Importance),] %>% mutate(Rank = row_number())

topVars <- subset(varImpScaffold, Rank %in% seq(from = 1, to = 6))[,1] %>% droplevels() %>% as.vector()

clusterSet <- NIH70KPredictions
clusterSet %<>% select(prob1, topVars) %>% subset(prob1 > .538) %>% select(topVars)

clusterSet %<>% scale(center = T, scale = T) %>% as.data.frame()

numClusters <- NbClust(clusterSet, distance = "euclidean", min.nc=2, max.nc=10, 
             method = "ward.D", index = "all")

## Optimal number of clusters is 3

distMat <- dist(x = clusterSet, method = "euclidean")

clust <- hclust(d = distMat, method = "ward.D")

sub_grp <- cutree(clust, k = 3)


fviz_cluster(list(data = clusterSet, cluster = sub_grp), geom = c("point"), choose.vars = c("SsOH", "SAdon"))
fviz_cluster(list(data = clusterSet, cluster = sub_grp), geom = c("point"), choose.vars = c("SsOH", "C-028"))
fviz_cluster(list(data = clusterSet, cluster = sub_grp), geom = c("point"), choose.vars = c("SsOH", "nPyridines"))
fviz_cluster(list(data = clusterSet, cluster = sub_grp), geom = c("point"), choose.vars = c("SsOH", "SpMin1_Bh(i)"))
fviz_cluster(list(data = clusterSet, cluster = sub_grp), geom = c("point"), choose.vars = c("SsOH", "CATS2D_00_NN"))
fviz_cluster(list(data = clusterSet, cluster = sub_grp), geom = c("point"), choose.vars = c("SAdon", "C-028"))
fviz_cluster(list(data = clusterSet, cluster = sub_grp), geom = c("point"), choose.vars = c("SAdon", "nPyridines"))
fviz_cluster(list(data = clusterSet, cluster = sub_grp), geom = c("point"), choose.vars = c("SAdon", "SpMin1_Bh(i)"))
fviz_cluster(list(data = clusterSet, cluster = sub_grp), geom = c("point"), choose.vars = c("SAdon", "CATS2D_00_NN"))
fviz_cluster(list(data = clusterSet, cluster = sub_grp), geom = c("point"), choose.vars = c("C-028", "nPyridines"))
fviz_cluster(list(data = clusterSet, cluster = sub_grp), geom = c("point"), choose.vars = c("C-028", "SpMin1_Bh(i)"))
fviz_cluster(list(data = clusterSet, cluster = sub_grp), geom = c("point"), choose.vars = c("C-028", "CATS2D_00_NN"))
fviz_cluster(list(data = clusterSet, cluster = sub_grp), geom = c("point"), choose.vars = c("nPyridines", "SpMin1_Bh(i)"))
fviz_cluster(list(data = clusterSet, cluster = sub_grp), geom = c("point"), choose.vars = c("SpMin1_Bh(i)", "CATS2D_00_NN"))


summaryStatsByClust <- NIH70KPredictions %>% subset(prob1 > .533) %>% cbind(sub_grp) %>% select(sub_grp, SsOH, SAdon, `C-028`, nPyridines, `SpMin1_Bh(i)`, CATS2D_00_NN) %>% group_by(sub_grp) %>% summarise_all(list(average = mean, max = max, min = min))

summaryStatsByClust.sorted <- summaryStatsByClust[ , order(names(summaryStatsByClust))] %>% select(-sub_grp) %>% cbind(unique(sub_grp), .)


DT::datatable(summaryStatsByClust.sorted, options = list(
  pageLength=50, scrollX='400px', order))

```